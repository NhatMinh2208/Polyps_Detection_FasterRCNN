{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "656177ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Msi-laptop\\Desktop\\Data_Science_Learn\\Polyps_Detection\\src\\utils\n",
      "['C:\\\\Users\\\\Msi-laptop\\\\Desktop\\\\Data_Science_Learn\\\\Polyps_Detection\\\\src', 'C:\\\\Users\\\\Msi-laptop\\\\anaconda3\\\\python38.zip', 'C:\\\\Users\\\\Msi-laptop\\\\anaconda3\\\\DLLs', 'C:\\\\Users\\\\Msi-laptop\\\\anaconda3\\\\lib', 'C:\\\\Users\\\\Msi-laptop\\\\anaconda3', '', 'C:\\\\Users\\\\Msi-laptop\\\\anaconda3\\\\lib\\\\site-packages', 'C:\\\\Users\\\\Msi-laptop\\\\anaconda3\\\\lib\\\\site-packages\\\\locket-0.2.1-py3.8.egg', 'C:\\\\Users\\\\Msi-laptop\\\\anaconda3\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\Msi-laptop\\\\anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\Msi-laptop\\\\anaconda3\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\Msi-laptop\\\\anaconda3\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Msi-laptop\\\\.ipython', 'C:\\\\Users\\\\Msi-laptop\\\\Desktop\\\\Data_Science_Learn\\\\Polyps_Detection\\\\src\\\\src\\\\utils', 'C:\\\\Users\\\\Msi-laptop\\\\Desktop\\\\Data_Science_Learn\\\\Polyps_Detection\\\\src\\\\utils', 'C:\\\\Users\\\\Msi-laptop\\\\Desktop\\\\Data_Science_Learn\\\\Polyps_Detection']\n",
      "['C:\\\\Users\\\\Msi-laptop\\\\Desktop\\\\Data_Science_Learn\\\\Polyps_Detection\\\\src', 'C:\\\\Users\\\\Msi-laptop\\\\anaconda3\\\\python38.zip', 'C:\\\\Users\\\\Msi-laptop\\\\anaconda3\\\\DLLs', 'C:\\\\Users\\\\Msi-laptop\\\\anaconda3\\\\lib', 'C:\\\\Users\\\\Msi-laptop\\\\anaconda3', '', 'C:\\\\Users\\\\Msi-laptop\\\\anaconda3\\\\lib\\\\site-packages', 'C:\\\\Users\\\\Msi-laptop\\\\anaconda3\\\\lib\\\\site-packages\\\\locket-0.2.1-py3.8.egg', 'C:\\\\Users\\\\Msi-laptop\\\\anaconda3\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\Msi-laptop\\\\anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\Msi-laptop\\\\anaconda3\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\Msi-laptop\\\\anaconda3\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Msi-laptop\\\\.ipython', 'C:\\\\Users\\\\Msi-laptop\\\\Desktop\\\\Data_Science_Learn\\\\Polyps_Detection\\\\src\\\\src\\\\utils', 'C:\\\\Users\\\\Msi-laptop\\\\Desktop\\\\Data_Science_Learn\\\\Polyps_Detection\\\\src\\\\utils', 'C:\\\\Users\\\\Msi-laptop\\\\Desktop\\\\Data_Science_Learn\\\\Polyps_Detection']\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-79192281fb12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIoU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparametrize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munparametrize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src.utils'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "#from src.utils import nms, IoU, parametrize, unparametrize\n",
    "module_path = os.path.abspath(os.path.join('./utils'))\n",
    "print(module_path)\n",
    "print(sys.path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "print(sys.path)\n",
    "\n",
    "from src.utils import nms, IoU, parametrize, unparametrize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76de71be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(nn.Module):\n",
    "    INPUT_SIZE = (1600, 800)\n",
    "    OUTPUT_SIZE = (100, 50)\n",
    "    OUTPUT_CELL_SIZE = float(INPUT_SIZE[0]) / float(OUTPUT_SIZE[0])\n",
    "\n",
    "    # anchors constants\n",
    "    ANCHORS_RATIOS = [0.25, 0.5, 0.9]\n",
    "    ANCHORS_SCALES = [4, 6, 8]\n",
    "\n",
    "    NUMBER_ANCHORS_WIDE = OUTPUT_SIZE[0]\n",
    "    NUMBER_ANCHORS_HEIGHT = OUTPUT_SIZE[1]\n",
    "\n",
    "    NEGATIVE_THRESHOLD = 0.3\n",
    "    POSITIVE_THRESHOLD = 0.6\n",
    "\n",
    "    ANCHOR_SAMPLING_SIZE = 256\n",
    "\n",
    "    NMS_THRESHOLD = 0.5\n",
    "    PRE_NMS_MAX_PROPOSALS = 6000\n",
    "    POST_NMS_MAX_PROPOSALS = 100\n",
    "\n",
    "    def __init__(self, in_dim):\n",
    "        super(RPN, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.anchor_dimensions = self.get_anchor_dimensions()\n",
    "        self.anchor_number = len(self.anchor_dimensions)\n",
    "        mid_layers = 1024\n",
    "        self.RPN_conv = nn.Conv2d(self.in_dim, mid_layers, 3, 1, 1)\n",
    "        # cls layer\n",
    "        self.cls_layer = nn.Conv2d(mid_layers, 2  * self.anchor_number, 1, 1, 0)\n",
    "        # reg_layer\n",
    "        self.reg_layer = nn.Conv2d(mid_layers, 4 * self.anchor_number, 1, 1, 0)\n",
    "\n",
    "        #initialize layers\n",
    "        torch.nn.init.normal_(self.RPN_conv.weight, std=0.01)\n",
    "        torch.nn.init.normal_(self.cls_layer.weight, std=0.01)\n",
    "        torch.nn.init.normal_(self.reg_layer.weight, std=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' Takes feature map as input'''\n",
    "        rpn_conv = F.relu(self.RPN_conv(x), inplace=True)\n",
    "        # permute dimensions\n",
    "        cls_output = self.cls_layer(rpn_conv).permute(0, 2, 3, 1).contiguous().view(1, -1, 2)\n",
    "        reg_output = self.reg_layer(rpn_conv).permute(0, 2, 3, 1).contiguous().view(1, -1, 4)\n",
    "\n",
    "        cls_output = F.softmax(cls_output.view(-1, 2), dim=1)\n",
    "        reg_output = reg_output.view(-1, 4)\n",
    "        return cls_output, reg_output\n",
    "\n",
    "    def get_target(self, bboxes):\n",
    "        anchors, filter_out = self.get_image_anchors()\n",
    "        truth_bbox, positives, negatives = self.get_positive_negative_anchors(anchors, bboxes)\n",
    "        reg_target = parametrize(anchors, truth_bbox)\n",
    "\n",
    "        n = len(anchors)\n",
    "        indices = np.array([i for i in range(n)])\n",
    "        selected_indices, positive_indices = self.get_selected_indices_sample(indices, positives, negatives)\n",
    "\n",
    "        cls_truth = np.zeros((n, 2))\n",
    "        cls_truth[np.arange(n), positives.astype(int)] = 1.0\n",
    "        return torch.from_numpy(reg_target), torch.from_numpy(cls_truth), selected_indices, positive_indices\n",
    "\n",
    "    def get_anchor_dimensions(self):\n",
    "        dimensions = []\n",
    "        for r in self.ANCHORS_RATIOS:\n",
    "            for s in self.ANCHORS_SCALES:\n",
    "                width = s * np.sqrt(r)\n",
    "                height = s * np.sqrt(1.0 / r)\n",
    "                dimensions.append((width, height))\n",
    "        return dimensions\n",
    "    # 9 anchors each pixel\n",
    "    # [(2.0, 8.0), (3.0, 12.0), (4.0, 16.0), (2.8284271247461903, 5.656854249492381), (4.242640687119286, 8.485281374238571), (5.6568\n",
    "    # 54249492381, 11.313708498984761), (3.794733192202055, 4.216370213557839), (5.692099788303082, 6.324555320336759), (7.5894663844\n",
    "    # 0411, 8.432740427115679)]\n",
    "\n",
    "    def get_anchors_at_position(self, pos):\n",
    "        # dimensions of anchors: (self.anchor_number, 4)\n",
    "        # each anchor is [xa, ya, xb, yb]\n",
    "        x, y = pos\n",
    "        anchors = np.zeros((self.anchor_number, 4))\n",
    "        for i in range(self.anchor_number):\n",
    "            center_x = self.OUTPUT_CELL_SIZE * (float(x) + 0.5)\n",
    "            center_y = self.OUTPUT_CELL_SIZE * (float(y) + 0.5)\n",
    "\n",
    "            width = self.anchor_dimensions[i][0] * self.OUTPUT_CELL_SIZE\n",
    "            height = self.anchor_dimensions[i][1] * self.OUTPUT_CELL_SIZE\n",
    "\n",
    "            top_x = center_x - width / 2.0\n",
    "            top_y = center_y - height / 2.0\n",
    "            anchors[i, :] = [top_x, top_y, top_x + width, top_y + height]\n",
    "        return anchors\n",
    "    # return 9 anchors in the input image\n",
    "\n",
    "    def get_proposals(self, reg, cls):\n",
    "        a, filter_out = self.get_image_anchors()\n",
    "        anchors = torch.from_numpy(a).float()\n",
    "        #Creates a Tensor from a numpy.ndarray.\n",
    "        bboxes = unparametrize(anchors, reg).reshape((-1, 4))\n",
    "        bboxes = bboxes[filter_out]\n",
    "        objects = torch.argmax(cls[filter_out], dim=1)\n",
    "        #Returns the indices of the maximum value of all elements in the input tensor.\n",
    "        #dim (int) â€“ the dimension to reduce. If None, the argmax of the flattened input is returned.\n",
    "        cls = cls.detach().numpy()\n",
    "        cls = cls[np.where(objects == 1)][:self.PRE_NMS_MAX_PROPOSALS]\n",
    "        bboxes = bboxes[np.where(objects == 1)][:self.PRE_NMS_MAX_PROPOSALS]\n",
    "        #get bboxes of anchors class \"object\"\n",
    "        keep = nms(bboxes.detach().numpy(), cls[:, 1].ravel(), self.NMS_THRESHOLD)[:self.POST_NMS_MAX_PROPOSALS]\n",
    "        return bboxes[keep]\n",
    "\n",
    "    def get_training_proposals(self, reg, cls):\n",
    "        a, filter_out = self.get_image_anchors()\n",
    "        anchors = torch.from_numpy(a).float()\n",
    "        bboxes = unparametrize(anchors, reg).reshape((-1, 4))\n",
    "        bboxes = bboxes[filter_out]\n",
    "        objects = torch.argmax(cls[filter_out], dim=1)\n",
    "\n",
    "        cls = cls.detach().numpy()\n",
    "        cls = cls[np.where(objects == 1)][:self.PRE_NMS_MAX_PROPOSALS]\n",
    "        bboxes = bboxes[np.where(objects == 1)][:self.PRE_NMS_MAX_PROPOSALS]\n",
    "        keep = nms(bboxes.detach().numpy(), cls[:, 1].ravel(), self.NMS_THRESHOLD)[:self.POST_NMS_MAX_PROPOSALS]\n",
    "        return bboxes[keep]\n",
    "\n",
    "    def get_image_anchors(self):\n",
    "        print('get_image_anchors')\n",
    "        anchors = np.zeros((self.NUMBER_ANCHORS_WIDE, self.NUMBER_ANCHORS_HEIGHT, self.anchor_number, 4))\n",
    "\n",
    "        for i in range(self.NUMBER_ANCHORS_WIDE):\n",
    "            for j in range(self.NUMBER_ANCHORS_HEIGHT):\n",
    "                anchors_pos = self.get_anchors_at_position((i, j))\n",
    "                anchors[i, j, :] = anchors_pos\n",
    "        anchors = anchors.reshape((-1, 4))\n",
    "        filter_out = (anchors[:, 0] < 0) | (anchors[:, 1] < 0) | (anchors[:, 2] > self.INPUT_SIZE[0]) | (anchors[:, 3] > self.INPUT_SIZE[1])\n",
    "        return anchors, np.where(~filter_out)\n",
    "    # return anchors.shape = (100x50x9, 4) <9 anchors with 4 params for each pixels>\n",
    "    # and ~ stand for \"NOT\" np.where(~filter_out) = (array([    0,     1,     2, ..., 44997, 44998, 44999], dtype=int64),)\n",
    "\n",
    "    def get_positive_negative_anchors(self, anchors, bboxes):\n",
    "        if not len(bboxes):\n",
    "            ious = np.zeros(anchors.shape[:3])\n",
    "            positives = ious > self.POSITIVE_THRESHOLD\n",
    "            negatives = ious < self.NEGATIVE_THRESHOLD\n",
    "            return np.array([]), positives, negatives\n",
    "\n",
    "        ious = np.zeros((anchors.shape[0], len(bboxes)))\n",
    "\n",
    "        # TODO improve speed with a real numpy formula\n",
    "        for i in range(ious.shape[0]):\n",
    "            for j in range(ious.shape[1]):\n",
    "                ious[i, j] = IoU(anchors[i], bboxes[j])\n",
    "        best_bbox_for_anchor = np.argmax(ious, axis=1)\n",
    "        best_anchor_for_bbox = np.argmax(ious, axis=0)\n",
    "        max_iou_per_anchor = np.amax(ious, axis=1)\n",
    "\n",
    "        # truth box for each anchor\n",
    "        truth_bbox = bboxes[best_bbox_for_anchor, :]\n",
    "\n",
    "        # Selecting all ious > POSITIVE_THRESHOLD\n",
    "        positives = max_iou_per_anchor > self.POSITIVE_THRESHOLD\n",
    "        # Adding max iou for each ground truth box\n",
    "        positives[best_anchor_for_bbox] = True\n",
    "        negatives = max_iou_per_anchor < self.NEGATIVE_THRESHOLD\n",
    "        return truth_bbox, positives, negatives\n",
    "\n",
    "    def get_selected_indices_sample(self, indices, positives, negatives):\n",
    "        positive_indices = indices[positives]\n",
    "        negative_indices = indices[negatives]\n",
    "        random_positives = np.random.permutation(positive_indices)[:self.ANCHOR_SAMPLING_SIZE // 2]\n",
    "        random_negatives = np.random.permutation(negative_indices)[:self.ANCHOR_SAMPLING_SIZE - len(random_positives)]\n",
    "        selected_indices = np.concatenate((random_positives, random_negatives))\n",
    "        return selected_indices, positive_indices\n",
    "\n",
    "    def get_positive_anchors(self, bboxes):\n",
    "        anchors, _ = self.get_image_anchors()\n",
    "        truth_bbox, positives, negatives = self.get_positive_negative_anchors(anchors, bboxes)\n",
    "\n",
    "        n = len(anchors)\n",
    "        indices = np.array([i for i in range(n)])\n",
    "        selected_indices, positive_indices = self.get_selected_indices_sample(indices, positives, negatives)\n",
    "        return anchors[positive_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a293ed97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_image_anchors\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'unparametrize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-cf7034782c87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrpn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRPN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrpn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_proposals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-c13b395fbaaa>\u001b[0m in \u001b[0;36mget_proposals\u001b[1;34m(self, reg, cls)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0manchors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;31m#Creates a Tensor from a numpy.ndarray.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mbboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munparametrize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         \u001b[0mbboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfilter_out\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mobjects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfilter_out\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unparametrize' is not defined"
     ]
    }
   ],
   "source": [
    "rpn = RPN(1024)\n",
    "x = rpn.get_proposals(1,1)\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
